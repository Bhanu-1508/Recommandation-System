# -*- coding: utf-8 -*-
"""Recomendatioon System Deploy.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AS4HWXTFiMrYvwMs93ypBqstHRV-ScrF
"""

#!pip install streamlit pandas scikit-learn
#!pip install scikit-surprise

import streamlit as st
import pandas as pd
from sklearn.model_selection import train_test_split
from surprise import Dataset, Reader, SVD, accuracy
from surprise.model_selection import cross_validate

# Function to merge uploaded files
def merge_files(user_file, book_file, rating_file):
    users_df = pd.read_csv(user_file)
    books_df = pd.read_csv(book_file)
    ratings_df = pd.read_csv(rating_file)
   # st.write('Users DataFrame Columns:', users_df.columns)
    #st.write('Books DataFrame Columns:', books_df.columns)
    #st.write('Ratings DataFrame Columns:', ratings_df.columns)
    
    merged_df = pd.merge(pd.merge(ratings_df, users_df, on='User-ID'), books_df, on='ISBN')
    #st.write('Merged DataFrame Columns:', merged_df.columns)
    return merged_df

def recommend_books(data, user_id):
    reader = Reader(rating_scale=(1, 10))
    dataset = Dataset.load_from_df(data[['User-ID', 'ISBN', 'rating_score']], reader)  # Update to correct column name   #'rating_score'    
    
    algo = SVD()
    algo.fit(trainset)
    
    all_books = data['ISBN'].unique()
    rated_books = data[data['User-ID'] == user_id]['ISBN'].unique()
    books_to_predict = [isbn for isbn in all_books if isbn not in rated_books]
    
    predictions = [algo.predict(user_id, isbn) for isbn in books_to_predict]
    top_n = sorted(predictions, key=lambda x: x.est, reverse=True)[:10]
    
    isbn_to_title = dict(zip(data['ISBN'], data['Book-Title']))
    recommendations = [(isbn_to_title.get(prediction.iid, 'Unknown Title'), prediction.iid, prediction.est) for prediction in top_n]
    
    return recommendations

# Function to train and make predictions
def recommend_books(data, user_id):
    reader = Reader(rating_scale=(1, 10))
    dataset = Dataset.load_from_df(data[['User-ID', 'ISBN', 'Book-Rating']], reader)  # Use actual column names
    trainset = dataset.build_full_trainset()
    
    algo = SVD()
    algo.fit(trainset)
    
    all_books = data['ISBN'].unique()
    rated_books = data[data['User-ID'] == user_id]['ISBN'].unique()
    books_to_predict = [isbn for isbn in all_books if isbn not in rated_books]
    
    predictions = [algo.predict(user_id, isbn) for isbn in books_to_predict]
    top_n = sorted(predictions, key=lambda x: x.est, reverse=True)[:10]
    
    isbn_to_title = dict(zip(data['ISBN'], data['Book-Title']))
    recommendations = [(isbn_to_title.get(prediction.iid, 'Unknown Title'), prediction.iid, prediction.est) for prediction in top_n]
    
    return recommendations

# Streamlit UI
st.title('Book Recommendation System')

uploaded_user_file = st.file_uploader("Upload User File", type="csv")
uploaded_book_file = st.file_uploader("Upload Book File", type="csv")
uploaded_rating_file = st.file_uploader("Upload Rating File", type="csv")

if uploaded_user_file and uploaded_book_file and uploaded_rating_file:
    with st.spinner('Processing files...'):
        data = merge_files(uploaded_user_file, uploaded_book_file, uploaded_rating_file)

        user_id = st.number_input('Enter User ID for Recommendations:', min_value=int(data['User-ID'].min()), max_value=int(data['User-ID'].max()))

        if st.button('Get Recommendations'):
            recommendations = recommend_books(data, user_id)
            st.write('Top 10 Recommendations:')
            for title, isbn, est in recommendations:
                st.write(f'Title: {title}, ISBN: {isbn}, Predicted Rating: {est:.2f}')

